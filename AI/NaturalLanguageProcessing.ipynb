{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaturalLanguageProcessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPJ+SJzJoXie3Nkj2hrUJPa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulFSherwood/python/blob/master/AI/NaturalLanguageProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqoAudfpg1RI"
      },
      "source": [
        "# **Bag of Words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KiCCBsIkMHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3130158-bda7-4ee0-a1da-6ec7713b987f"
      },
      "source": [
        "vocab = {}  # maps word to integer representing it\n",
        "word_encoding = 1\n",
        "def bag_of_words(text):\n",
        "  global word_encoding\n",
        "\n",
        "  words = text.lower().split(\" \")  # create a list of all of the words in the text, well assume there is no grammar in our text for this example\n",
        "  bag = {}  # stores all of the encodings and their frequency\n",
        "\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      encoding = vocab[word]  # get encoding from vocab\n",
        "    else:\n",
        "      vocab[word] = word_encoding\n",
        "      encoding = word_encoding\n",
        "      word_encoding += 1\n",
        "    \n",
        "    if encoding in bag:\n",
        "      bag[encoding] += 1\n",
        "    else:\n",
        "      bag[encoding] = 1\n",
        "  \n",
        "  return bag\n",
        "\n",
        "text = \"this is a test to see if this test will work is is test a a\"\n",
        "bag = bag_of_words(text)\n",
        "print(bag)\n",
        "print(vocab)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n",
            "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lFGJeL_hB4Z",
        "outputId": "2b9f69e3-f9ad-45c0-9dcd-7b00c2173565"
      },
      "source": [
        "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\r\n",
        "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\r\n",
        "\r\n",
        "pos_bag = bag_of_words(positive_review)\r\n",
        "neg_bag = bag_of_words(negative_review)\r\n",
        "\r\n",
        "print(\"Positive:\", pos_bag)\r\n",
        "print(\"Negative:\", neg_bag)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive: {10: 1, 11: 1, 12: 1, 13: 1, 14: 2, 15: 1, 5: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1}\n",
            "Negative: {10: 1, 11: 1, 12: 1, 13: 1, 14: 2, 15: 1, 5: 1, 16: 1, 21: 1, 18: 1, 19: 1, 20: 1, 17: 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeL6BinQhElo"
      },
      "source": [
        "# **Integer Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0Pg_OfmhF9m",
        "outputId": "b40b4dab-47d0-4952-f1a4-72b84d2a5fcf"
      },
      "source": [
        "vocab = {}  \r\n",
        "word_encoding = 1\r\n",
        "def one_hot_encoding(text):\r\n",
        "  global word_encoding\r\n",
        "\r\n",
        "  words = text.lower().split(\" \") \r\n",
        "  encoding = []  \r\n",
        "\r\n",
        "  for word in words:\r\n",
        "    if word in vocab:\r\n",
        "      code = vocab[word]  \r\n",
        "      encoding.append(code) \r\n",
        "    else:\r\n",
        "      vocab[word] = word_encoding\r\n",
        "      encoding.append(word_encoding)\r\n",
        "      word_encoding += 1\r\n",
        "  \r\n",
        "  return encoding\r\n",
        "\r\n",
        "text = \"this is a test to see if this test will work is is test a a\"\r\n",
        "encoding = one_hot_encoding(text)\r\n",
        "print(encoding)\r\n",
        "print(vocab)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 1, 4, 8, 9, 2, 2, 4, 3, 3]\n",
            "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsH-qtahhNEA",
        "outputId": "07b98608-56b7-41bd-fb88-b45fad9d357f"
      },
      "source": [
        "positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\r\n",
        "negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\r\n",
        "\r\n",
        "pos_encode = one_hot_encoding(positive_review)\r\n",
        "neg_encode = one_hot_encoding(negative_review)\r\n",
        "\r\n",
        "print(\"Positive:\", pos_encode)\r\n",
        "print(\"Negative:\", neg_encode)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive: [10, 11, 12, 13, 14, 15, 5, 16, 17, 18, 19, 14, 20, 21]\n",
            "Negative: [10, 11, 12, 13, 14, 15, 5, 16, 21, 18, 19, 14, 20, 17]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssHB8p_Ihan_"
      },
      "source": [
        "# **Recurrent Neural Networks (RNN's)**\r\n",
        "**LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6P59IKPhfvA",
        "outputId": "2d1eec9c-3813-46d8-8a3e-45d7b083f1ee"
      },
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\r\n",
        "from keras.datasets import imdb\r\n",
        "from keras.preprocessing import sequence\r\n",
        "import keras\r\n",
        "import tensorflow as tf\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "VOCAB_SIZE = 88584\r\n",
        "\r\n",
        "MAXLEN = 250\r\n",
        "BATCH_SIZE = 64\r\n",
        "\r\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxWNB22Phk_K",
        "outputId": "83947111-ca06-4e03-9849-c598935e8590",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Lets look at one review\r\n",
        "train_data[1]"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 194,\n",
              " 1153,\n",
              " 194,\n",
              " 8255,\n",
              " 78,\n",
              " 228,\n",
              " 5,\n",
              " 6,\n",
              " 1463,\n",
              " 4369,\n",
              " 5012,\n",
              " 134,\n",
              " 26,\n",
              " 4,\n",
              " 715,\n",
              " 8,\n",
              " 118,\n",
              " 1634,\n",
              " 14,\n",
              " 394,\n",
              " 20,\n",
              " 13,\n",
              " 119,\n",
              " 954,\n",
              " 189,\n",
              " 102,\n",
              " 5,\n",
              " 207,\n",
              " 110,\n",
              " 3103,\n",
              " 21,\n",
              " 14,\n",
              " 69,\n",
              " 188,\n",
              " 8,\n",
              " 30,\n",
              " 23,\n",
              " 7,\n",
              " 4,\n",
              " 249,\n",
              " 126,\n",
              " 93,\n",
              " 4,\n",
              " 114,\n",
              " 9,\n",
              " 2300,\n",
              " 1523,\n",
              " 5,\n",
              " 647,\n",
              " 4,\n",
              " 116,\n",
              " 9,\n",
              " 35,\n",
              " 8163,\n",
              " 4,\n",
              " 229,\n",
              " 9,\n",
              " 340,\n",
              " 1322,\n",
              " 4,\n",
              " 118,\n",
              " 9,\n",
              " 4,\n",
              " 130,\n",
              " 4901,\n",
              " 19,\n",
              " 4,\n",
              " 1002,\n",
              " 5,\n",
              " 89,\n",
              " 29,\n",
              " 952,\n",
              " 46,\n",
              " 37,\n",
              " 4,\n",
              " 455,\n",
              " 9,\n",
              " 45,\n",
              " 43,\n",
              " 38,\n",
              " 1543,\n",
              " 1905,\n",
              " 398,\n",
              " 4,\n",
              " 1649,\n",
              " 26,\n",
              " 6853,\n",
              " 5,\n",
              " 163,\n",
              " 11,\n",
              " 3215,\n",
              " 10156,\n",
              " 4,\n",
              " 1153,\n",
              " 9,\n",
              " 194,\n",
              " 775,\n",
              " 7,\n",
              " 8255,\n",
              " 11596,\n",
              " 349,\n",
              " 2637,\n",
              " 148,\n",
              " 605,\n",
              " 15358,\n",
              " 8003,\n",
              " 15,\n",
              " 123,\n",
              " 125,\n",
              " 68,\n",
              " 23141,\n",
              " 6853,\n",
              " 15,\n",
              " 349,\n",
              " 165,\n",
              " 4362,\n",
              " 98,\n",
              " 5,\n",
              " 4,\n",
              " 228,\n",
              " 9,\n",
              " 43,\n",
              " 36893,\n",
              " 1157,\n",
              " 15,\n",
              " 299,\n",
              " 120,\n",
              " 5,\n",
              " 120,\n",
              " 174,\n",
              " 11,\n",
              " 220,\n",
              " 175,\n",
              " 136,\n",
              " 50,\n",
              " 9,\n",
              " 4373,\n",
              " 228,\n",
              " 8255,\n",
              " 5,\n",
              " 25249,\n",
              " 656,\n",
              " 245,\n",
              " 2350,\n",
              " 5,\n",
              " 4,\n",
              " 9837,\n",
              " 131,\n",
              " 152,\n",
              " 491,\n",
              " 18,\n",
              " 46151,\n",
              " 32,\n",
              " 7464,\n",
              " 1212,\n",
              " 14,\n",
              " 9,\n",
              " 6,\n",
              " 371,\n",
              " 78,\n",
              " 22,\n",
              " 625,\n",
              " 64,\n",
              " 1382,\n",
              " 9,\n",
              " 8,\n",
              " 168,\n",
              " 145,\n",
              " 23,\n",
              " 4,\n",
              " 1690,\n",
              " 15,\n",
              " 16,\n",
              " 4,\n",
              " 1355,\n",
              " 5,\n",
              " 28,\n",
              " 6,\n",
              " 52,\n",
              " 154,\n",
              " 462,\n",
              " 33,\n",
              " 89,\n",
              " 78,\n",
              " 285,\n",
              " 16,\n",
              " 145,\n",
              " 95]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uBszD-HhnGs"
      },
      "source": [
        "train_data = sequence.pad_sequences(train_data, MAXLEN)\r\n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN)\r\n",
        "# train_data[1]"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8jqqhKHhxNV"
      },
      "source": [
        "# **Creating the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ObuIZR_hvRY"
      },
      "source": [
        "model = tf.keras.Sequential([\r\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\r\n",
        "    tf.keras.layers.LSTM(32),\r\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\r\n",
        "])"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1_WOS6ph3P-",
        "outputId": "33e9bc11-501b-49a8-a512-b33f2cff6f8c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, None, 32)          2834688   \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTS6_YTEh7SI"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9f14gt-h8Gi",
        "outputId": "613745a2-45f7-4142-e3af-79923e1f8516"
      },
      "source": [
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\r\n",
        "\r\n",
        "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 18s 26ms/step - loss: 0.5275 - acc: 0.7342 - val_loss: 0.3228 - val_acc: 0.8692\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.2394 - acc: 0.9103 - val_loss: 0.2679 - val_acc: 0.8904\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.1810 - acc: 0.9344 - val_loss: 0.2696 - val_acc: 0.8914\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.1441 - acc: 0.9482 - val_loss: 0.2966 - val_acc: 0.8932\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.1253 - acc: 0.9590 - val_loss: 0.2921 - val_acc: 0.8882\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.1102 - acc: 0.9628 - val_loss: 0.2996 - val_acc: 0.8796\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.0941 - acc: 0.9696 - val_loss: 0.3970 - val_acc: 0.8654\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.0821 - acc: 0.9753 - val_loss: 0.3553 - val_acc: 0.8820\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.0746 - acc: 0.9753 - val_loss: 0.3820 - val_acc: 0.8724\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 16s 25ms/step - loss: 0.0637 - acc: 0.9799 - val_loss: 0.3722 - val_acc: 0.8860\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N62lPoKfh97p",
        "outputId": "e2e35fd1-d879-4a56-a930-88d400476ac2"
      },
      "source": [
        "results = model.evaluate(test_data, test_labels)\r\n",
        "print(results)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 5s 7ms/step - loss: 0.4820 - acc: 0.8474\n",
            "[0.4819587171077728, 0.8473600149154663]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po4coV_MiAu4"
      },
      "source": [
        "**Making Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln1rjgJtiBm_",
        "outputId": "92e0c879-d188-46a6-d682-9f17a82390ac"
      },
      "source": [
        "word_index = imdb.get_word_index()\r\n",
        "\r\n",
        "def encode_text(text):\r\n",
        "  tokens = keras.preprocessing.text.text_to_word_sequence(text)\r\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\r\n",
        "  return sequence.pad_sequences([tokens], MAXLEN)[0]\r\n",
        "\r\n",
        "text = \"that movie was just amazing, so amazing\"\r\n",
        "encoded = encode_text(text)\r\n",
        "print(encoded)\r\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2Jhg5XYldR8"
      },
      "source": [
        "#print(word_index)\r\n",
        "# for bacon in word_index:\r\n",
        "#     print(bacon)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvJ8ktltiDUy",
        "outputId": "d52f756e-e667-4820-ebd8-e21192b6547d"
      },
      "source": [
        "# while were at it lets make a decode function\r\n",
        "\r\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\r\n",
        "\r\n",
        "def decode_integers(integers):\r\n",
        "    PAD = 0\r\n",
        "    text = \"\"\r\n",
        "    for num in integers:\r\n",
        "      if num != PAD:\r\n",
        "        text += reverse_word_index[num] + \" \"\r\n",
        "\r\n",
        "    return text[:-1]\r\n",
        "  \r\n",
        "print(decode_integers(encoded))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "that movie was just amazing so amazing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQB3hx3miFNf",
        "outputId": "6fdb32d5-78e8-43f8-efec-6c5cfb31fcc5"
      },
      "source": [
        "# now time to make a prediction\r\n",
        "\r\n",
        "def predict(text):\r\n",
        "  encoded_text = encode_text(text)\r\n",
        "  pred = np.zeros((1,250))\r\n",
        "  pred[0] = encoded_text\r\n",
        "  result = model.predict(pred) \r\n",
        "  print(result[0])\r\n",
        "\r\n",
        "positive_review = \"That movie was great amazing awesome! really loved it and would great watch it again because it was amazingly great\"\r\n",
        "predict(positive_review)\r\n",
        "\r\n",
        "negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\r\n",
        "predict(negative_review)\r\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.82697695]\n",
            "[0.20299049]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TObxQeSDjycV"
      },
      "source": [
        "# **RNN Play Generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys-1ugl0qLR2",
        "outputId": "7b702bc9-cc7d-4f41-df66-c349af23b2e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\r\n",
        "from keras.preprocessing import sequence\r\n",
        "import keras\r\n",
        "import tensorflow as tf\r\n",
        "import os\r\n",
        "import numpy as np"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6PL0NGIqOTU"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaMBekALqRNC"
      },
      "source": [
        "# **Loading Your Own Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8I5v1XGqSLo"
      },
      "source": [
        "# from google.colab import files\r\n",
        "# path_to_file = list(files.upload().keys())[0]"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1rrzFM9qWSS"
      },
      "source": [
        "**Read Contents of File**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAQ0reLpqXMZ",
        "outputId": "e8d00797-cb9f-462a-83f6-d32ef26ea51b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Read, then decode for py2 compat.\r\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\r\n",
        "# length of text is the number of characters in it\r\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp5chWQAqY2_",
        "outputId": "77f55c8b-f5a5-4b12-b23a-518173885990",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Take a look at the first 250 characters in text\r\n",
        "print(text[:250])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MalEalY7qlrk"
      },
      "source": [
        "**Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY26RE74qma-"
      },
      "source": [
        "vocab = sorted(set(text))\r\n",
        "# Creating a mapping from unique characters to indices\r\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\r\n",
        "idx2char = np.array(vocab)\r\n",
        "\r\n",
        "def text_to_int(text):\r\n",
        "  return np.array([char2idx[c] for c in text])\r\n",
        "\r\n",
        "text_as_int = text_to_int(text)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwJc8MDZqohm",
        "outputId": "45360ecf-6d4b-4d71-9437-170c89c42140",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# lets look at how part of our text is encoded\r\n",
        "print(\"Text:\", text[:13])\r\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAbs_IOZqqGM",
        "outputId": "c7c09c37-951e-49b9-bfab-84af11461379",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def int_to_text(ints):\r\n",
        "  try:\r\n",
        "    ints = ints.numpy()\r\n",
        "  except:\r\n",
        "    pass\r\n",
        "  return ''.join(idx2char[ints])\r\n",
        "\r\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehoxQEo1qs7Q"
      },
      "source": [
        "**Creating Training Examples**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yquOIEZ3qw_Q"
      },
      "source": [
        "seq_length = 100  # length of sequence for a training example\r\n",
        "examples_per_epoch = len(text)//(seq_length+1)\r\n",
        "\r\n",
        "# Create training examples / targets\r\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDkjZsFOqzU-"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU7_BjwFq1ZK"
      },
      "source": [
        "def split_input_target(chunk):  # for the example: hello\r\n",
        "    input_text = chunk[:-1]  # hell\r\n",
        "    target_text = chunk[1:]  # ello\r\n",
        "    return input_text, target_text  # hell, ello\r\n",
        "\r\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7p5H49zq2_M",
        "outputId": "3a9309e3-03ad-4895-ac7d-b8864e452e57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for x, y in dataset.take(2):\r\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\r\n",
        "  print(\"INPUT\")\r\n",
        "  print(int_to_text(x))\r\n",
        "  print(\"\\nOUTPUT\")\r\n",
        "  print(int_to_text(y))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P3YQpHyq5iW"
      },
      "source": [
        "BATCH_SIZE = 64\r\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\r\n",
        "EMBEDDING_DIM = 256\r\n",
        "RNN_UNITS = 1024\r\n",
        "\r\n",
        "# Buffer size to shuffle the dataset\r\n",
        "# (TF data is designed to work with possibly infinite sequences,\r\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\r\n",
        "# it maintains a buffer in which it shuffles elements).\r\n",
        "BUFFER_SIZE = 10000\r\n",
        "\r\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftK02Ssrq77u"
      },
      "source": [
        "# Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cxXA1Sdq99g",
        "outputId": "8659feb5-01ec-4d06-cc69-cb1dc38333fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\r\n",
        "  model = tf.keras.Sequential([\r\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\r\n",
        "                              batch_input_shape=[batch_size, None]),\r\n",
        "    tf.keras.layers.LSTM(rnn_units,\r\n",
        "                        return_sequences=True,\r\n",
        "                        stateful=True,\r\n",
        "                        recurrent_initializer='glorot_uniform'),\r\n",
        "    tf.keras.layers.Dense(vocab_size)\r\n",
        "  ])\r\n",
        "  return model\r\n",
        "\r\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\r\n",
        "model.summary()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wrLmlXCrAcS"
      },
      "source": [
        "# Creating a Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6ta1B5crCe0",
        "outputId": "0dad29ad-8c9f-4a3f-94c0-79911bd7168e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\r\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\r\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOs7O7QWrDCz",
        "outputId": "07328bb6-cf30-439d-d85f-5d00098f3ddb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\r\n",
        "print(len(example_batch_predictions))\r\n",
        "print(example_batch_predictions)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-1.62505324e-03  3.59386881e-03 -6.65399421e-06 ... -1.15153252e-03\n",
            "   -6.55478612e-03 -3.47776012e-03]\n",
            "  [-5.48006082e-03  6.02463307e-03 -9.94449016e-04 ...  1.77889329e-03\n",
            "    1.64334627e-03 -3.04601551e-03]\n",
            "  [-1.40967348e-03  4.03190730e-03  3.88384052e-03 ...  1.12839509e-04\n",
            "    2.53246399e-04 -3.50410584e-03]\n",
            "  ...\n",
            "  [-2.28423253e-03  2.12398381e-03  4.57549980e-03 ...  3.09996936e-03\n",
            "   -1.15486942e-02 -1.62990019e-03]\n",
            "  [-4.38899733e-03  4.94328979e-03  3.64788598e-03 ...  1.30360923e-03\n",
            "   -1.55024203e-02 -3.20083252e-03]\n",
            "  [-2.02452950e-03 -8.09304067e-04  1.01950183e-03 ...  5.02040423e-03\n",
            "   -6.65466907e-03 -2.85698520e-03]]\n",
            "\n",
            " [[ 2.85359588e-03 -1.08860189e-03  3.80810164e-03 ... -9.20211198e-04\n",
            "   -2.46711599e-04 -1.54756568e-03]\n",
            "  [-1.09100039e-03  5.52108977e-03 -1.48369896e-03 ...  1.81643502e-03\n",
            "   -4.14534938e-03  3.33285891e-03]\n",
            "  [-3.05662001e-03  5.62557532e-03 -8.21529306e-04 ... -5.21969749e-03\n",
            "   -6.85834046e-03  4.90637030e-04]\n",
            "  ...\n",
            "  [-8.12934339e-03  8.47647316e-04  4.71982174e-03 ... -4.13957227e-04\n",
            "   -5.29199932e-03  4.57038917e-03]\n",
            "  [-4.33456432e-03  2.42680381e-03  5.52352564e-03 ...  2.23514577e-03\n",
            "   -2.22932734e-03  3.34671629e-03]\n",
            "  [-2.87004118e-03  5.36314910e-03  3.15193855e-03 ...  2.42600590e-03\n",
            "    6.58688601e-04  1.06750615e-02]]\n",
            "\n",
            " [[ 6.18886610e-04  1.59341272e-03 -6.50478876e-04 ...  1.59778597e-03\n",
            "    3.52922664e-03  8.10752343e-03]\n",
            "  [ 3.81163601e-03 -4.50408843e-05  3.75208794e-03 ...  8.63702036e-04\n",
            "    1.21010526e-03  4.81800362e-03]\n",
            "  [ 9.13752534e-04  2.30525155e-03  5.99201396e-03 ... -2.32169963e-03\n",
            "   -1.29024126e-03  3.88858980e-03]\n",
            "  ...\n",
            "  [-8.12781975e-03  3.29428562e-03 -2.37001269e-03 ...  5.06753847e-03\n",
            "   -6.78920839e-03  1.36624184e-03]\n",
            "  [-7.98235461e-03  7.00165331e-03 -2.56823865e-03 ...  1.74044934e-03\n",
            "   -1.11795291e-02 -1.82006019e-03]\n",
            "  [-1.05100852e-02  9.52378754e-03 -3.55050061e-03 ...  3.06741660e-03\n",
            "   -1.47419132e-03 -1.41936936e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 3.60016478e-03  3.08058830e-03  1.17087632e-03 ...  2.39799963e-03\n",
            "   -8.41624977e-04  6.14634715e-04]\n",
            "  [ 9.49931098e-04  6.29863236e-03  1.22175494e-03 ...  4.60597133e-04\n",
            "   -7.57069699e-03 -2.82987719e-03]\n",
            "  [-1.46291114e-03  7.50763342e-03  3.25475773e-03 ... -5.01916045e-03\n",
            "   -6.65487163e-03 -1.02218892e-03]\n",
            "  ...\n",
            "  [ 1.91640039e-03 -1.06259459e-03 -3.28585715e-03 ... -5.56582818e-05\n",
            "   -2.26087589e-03  6.38699904e-03]\n",
            "  [ 4.63176053e-03  2.29684683e-03  8.48985161e-04 ...  3.97065480e-04\n",
            "   -9.25938599e-04  3.09334463e-03]\n",
            "  [ 5.25652058e-03  1.59018452e-03 -1.55026163e-03 ... -5.00170048e-04\n",
            "    1.22681865e-03  4.42905957e-03]]\n",
            "\n",
            " [[-1.62505324e-03  3.59386881e-03 -6.65399421e-06 ... -1.15153252e-03\n",
            "   -6.55478612e-03 -3.47776012e-03]\n",
            "  [-5.08104125e-03 -2.31980754e-04  3.28842457e-03 ...  4.65452159e-03\n",
            "   -1.38815423e-03 -7.72932079e-04]\n",
            "  [-5.54782525e-03 -5.08918753e-03  2.51266710e-03 ...  7.64913950e-03\n",
            "    1.39034051e-03 -6.73328992e-04]\n",
            "  ...\n",
            "  [-7.59452116e-03 -8.46604537e-03  1.20602059e-03 ...  1.07619427e-02\n",
            "   -7.52717815e-03 -1.36639853e-03]\n",
            "  [-3.70238326e-03 -5.94638009e-03 -2.30950699e-03 ...  1.72503740e-02\n",
            "   -6.99643744e-03  4.45584394e-03]\n",
            "  [-3.65554611e-03 -1.53174717e-03 -1.82172051e-03 ...  1.12746572e-02\n",
            "   -1.12517551e-02  5.77421510e-04]]\n",
            "\n",
            " [[ 1.49005651e-03 -7.53301778e-04 -7.97642220e-04 ...  3.06293531e-03\n",
            "    1.05314958e-03  5.01034723e-04]\n",
            "  [ 2.07592896e-03 -2.00459431e-03 -4.25578840e-03 ...  1.03793107e-02\n",
            "   -7.42825912e-04  5.44567872e-03]\n",
            "  [-1.45122409e-04  1.25035760e-04 -3.39116156e-03 ...  5.49057592e-03\n",
            "   -6.68047089e-03  1.05667859e-03]\n",
            "  ...\n",
            "  [-6.01976085e-03  1.00366082e-02  3.08994693e-03 ...  6.47485582e-03\n",
            "   -1.90721243e-04  1.15346082e-03]\n",
            "  [-2.12614541e-03  7.65140960e-03  7.01174559e-03 ...  3.64076253e-03\n",
            "   -1.43883890e-03  2.57951673e-04]\n",
            "  [ 1.35847321e-03  2.28067744e-03  7.25643244e-03 ...  4.48650867e-03\n",
            "   -9.85500123e-03 -1.40781316e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bo1-4NorFO7",
        "outputId": "db8d0c11-de73-4bdb-83a5-88926d066f7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# lets examine one prediction\r\n",
        "pred = example_batch_predictions[0]\r\n",
        "print(len(pred))\r\n",
        "print(pred)\r\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-1.6250532e-03  3.5938688e-03 -6.6539942e-06 ... -1.1515325e-03\n",
            "  -6.5547861e-03 -3.4777601e-03]\n",
            " [-5.4800608e-03  6.0246331e-03 -9.9444902e-04 ...  1.7788933e-03\n",
            "   1.6433463e-03 -3.0460155e-03]\n",
            " [-1.4096735e-03  4.0319073e-03  3.8838405e-03 ...  1.1283951e-04\n",
            "   2.5324640e-04 -3.5041058e-03]\n",
            " ...\n",
            " [-2.2842325e-03  2.1239838e-03  4.5754998e-03 ...  3.0999694e-03\n",
            "  -1.1548694e-02 -1.6299002e-03]\n",
            " [-4.3889973e-03  4.9432898e-03  3.6478860e-03 ...  1.3036092e-03\n",
            "  -1.5502420e-02 -3.2008325e-03]\n",
            " [-2.0245295e-03 -8.0930407e-04  1.0195018e-03 ...  5.0204042e-03\n",
            "  -6.6546691e-03 -2.8569852e-03]], shape=(100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgv7cX9PrG37",
        "outputId": "14ad9381-2582-4244-a0d1-66ee9f6935ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# and finally well look at a prediction at the first timestep\r\n",
        "time_pred = pred[0]\r\n",
        "print(len(time_pred))\r\n",
        "print(time_pred)\r\n",
        "# and of course its 65 values representing the probabillity of each character occuring next"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-1.6250532e-03  3.5938688e-03 -6.6539942e-06  5.9443144e-03\n",
            "  1.5555980e-03 -6.2354421e-04 -3.1057408e-03 -1.0487026e-03\n",
            " -1.2660190e-03  3.3148478e-03 -4.7197449e-03 -9.3634659e-04\n",
            "  2.1885730e-04 -2.7516764e-03  1.7832371e-04  4.4216635e-03\n",
            " -8.6925516e-04  5.4131807e-03 -3.5724503e-03  1.4334418e-03\n",
            "  6.0079182e-03 -7.3309750e-03  1.7004315e-03  1.2101134e-04\n",
            "  8.2305754e-03 -2.2327718e-03 -9.2433882e-04  3.2517659e-03\n",
            " -7.8073703e-05 -7.8498921e-04 -5.2477373e-04 -1.2755372e-03\n",
            " -8.6616967e-03 -3.9207037e-03 -1.7413723e-03 -3.4206070e-03\n",
            "  3.1163287e-04  1.5060733e-03 -9.3835348e-04  3.5825623e-03\n",
            "  4.2889547e-03  3.4167967e-03  8.4788958e-03  3.6620616e-03\n",
            " -7.5466279e-04 -1.3324963e-03  3.3596891e-03 -3.9279519e-04\n",
            "  1.9162174e-03 -1.1268323e-03 -2.0255223e-03  1.0944073e-02\n",
            "  2.4563367e-03 -5.0793483e-04 -1.2503150e-03 -2.9480830e-04\n",
            "  1.9051391e-03  1.1698531e-03 -4.3812487e-03  2.4237712e-03\n",
            " -4.1134600e-03  8.1277732e-04 -1.1515325e-03 -6.5547861e-03\n",
            " -3.4777601e-03], shape=(65,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoU6euZorI2b",
        "outputId": "96bc9e01-9634-46c0-b27e-a7a644de1433",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\r\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\r\n",
        "\r\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\r\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\r\n",
        "predicted_chars = int_to_text(sampled_indices)\r\n",
        "\r\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"h?DsjjMH3SVsHDXM$PvSWJojMP!BOUkBMTB N-VTkwsUOwbMwxahmkSABYq-.sTXwASQ$j.sPKkJ3Ns!AtsdyMaWY:il?V'rvZtE\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlF6CMsxrURz"
      },
      "source": [
        "def loss(labels, logits):\r\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpOMQ_99rVjQ"
      },
      "source": [
        "**Compiling the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTU_ctQHrZSU"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1_m1Uxnrby0"
      },
      "source": [
        "**Creating Checkpoints**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vYrkmderZ_g"
      },
      "source": [
        "# Directory where the checkpoints will be saved\r\n",
        "checkpoint_dir = './training_checkpoints'\r\n",
        "# Name of the checkpoint files\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n",
        "\r\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\r\n",
        "    filepath=checkpoint_prefix,\r\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elpQUppQrjxA"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeu5z71ErlqY",
        "outputId": "4224024a-b0e7-4063-ea1f-8f842fcc3209",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "history = model.fit(data, epochs=150, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "172/172 [==============================] - 16s 80ms/step - loss: 3.0371\n",
            "Epoch 2/150\n",
            "172/172 [==============================] - 15s 79ms/step - loss: 1.9738\n",
            "Epoch 3/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 1.6797\n",
            "Epoch 4/150\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 1.5196\n",
            "Epoch 5/150\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 1.4312\n",
            "Epoch 6/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 1.3679\n",
            "Epoch 7/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 1.3174\n",
            "Epoch 8/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 1.2760\n",
            "Epoch 9/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 1.2378\n",
            "Epoch 10/150\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 1.2017\n",
            "Epoch 11/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 1.1607\n",
            "Epoch 12/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 1.1263\n",
            "Epoch 13/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 1.0869\n",
            "Epoch 14/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 1.0466\n",
            "Epoch 15/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 1.0027\n",
            "Epoch 16/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.9633\n",
            "Epoch 17/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.9200\n",
            "Epoch 18/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.8777\n",
            "Epoch 19/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.8367\n",
            "Epoch 20/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.7972\n",
            "Epoch 21/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.7605\n",
            "Epoch 22/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.7283\n",
            "Epoch 23/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.6951\n",
            "Epoch 24/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.6669\n",
            "Epoch 25/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.6378\n",
            "Epoch 26/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.6141\n",
            "Epoch 27/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.5922\n",
            "Epoch 28/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.5741\n",
            "Epoch 29/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.5583\n",
            "Epoch 30/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.5418\n",
            "Epoch 31/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.5264\n",
            "Epoch 32/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.5157\n",
            "Epoch 33/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.5027\n",
            "Epoch 34/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.4952\n",
            "Epoch 35/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.4843\n",
            "Epoch 36/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.4769\n",
            "Epoch 37/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.4642\n",
            "Epoch 38/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.4618\n",
            "Epoch 39/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.4519\n",
            "Epoch 40/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.4483\n",
            "Epoch 41/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.4415\n",
            "Epoch 42/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.4358\n",
            "Epoch 43/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.4352\n",
            "Epoch 44/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.4315\n",
            "Epoch 45/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.4273\n",
            "Epoch 46/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.4253\n",
            "Epoch 47/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.4204\n",
            "Epoch 48/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.4191\n",
            "Epoch 49/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.4169\n",
            "Epoch 50/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.4118\n",
            "Epoch 51/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.4111\n",
            "Epoch 52/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.4078\n",
            "Epoch 53/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.4052\n",
            "Epoch 54/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.4072\n",
            "Epoch 55/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.4043\n",
            "Epoch 56/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.4025\n",
            "Epoch 57/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.4003\n",
            "Epoch 58/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3990\n",
            "Epoch 59/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3954\n",
            "Epoch 60/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3967\n",
            "Epoch 61/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3948\n",
            "Epoch 62/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3916\n",
            "Epoch 63/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3907\n",
            "Epoch 64/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3931\n",
            "Epoch 65/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3882\n",
            "Epoch 66/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3847\n",
            "Epoch 67/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3901\n",
            "Epoch 68/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3854\n",
            "Epoch 69/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3866\n",
            "Epoch 70/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3867\n",
            "Epoch 71/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3855\n",
            "Epoch 72/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3843\n",
            "Epoch 73/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3823\n",
            "Epoch 74/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3842\n",
            "Epoch 75/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3812\n",
            "Epoch 76/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3833\n",
            "Epoch 77/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3829\n",
            "Epoch 78/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3805\n",
            "Epoch 79/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3847\n",
            "Epoch 80/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3827\n",
            "Epoch 81/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3767\n",
            "Epoch 82/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3795\n",
            "Epoch 83/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3785\n",
            "Epoch 84/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3769\n",
            "Epoch 85/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3776\n",
            "Epoch 86/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3761\n",
            "Epoch 87/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3779\n",
            "Epoch 88/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3755\n",
            "Epoch 89/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3751\n",
            "Epoch 90/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3766\n",
            "Epoch 91/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3761\n",
            "Epoch 92/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3762\n",
            "Epoch 93/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3785\n",
            "Epoch 94/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3784\n",
            "Epoch 95/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3766\n",
            "Epoch 96/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3789\n",
            "Epoch 97/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3754\n",
            "Epoch 98/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3759\n",
            "Epoch 99/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3772\n",
            "Epoch 100/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3783\n",
            "Epoch 101/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3810\n",
            "Epoch 102/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3768\n",
            "Epoch 103/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3772\n",
            "Epoch 104/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3765\n",
            "Epoch 105/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3732\n",
            "Epoch 106/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3729\n",
            "Epoch 107/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3722\n",
            "Epoch 108/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3733\n",
            "Epoch 109/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3733\n",
            "Epoch 110/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3757\n",
            "Epoch 111/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3776\n",
            "Epoch 112/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3754\n",
            "Epoch 113/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3790\n",
            "Epoch 114/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3803\n",
            "Epoch 115/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3807\n",
            "Epoch 116/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3819\n",
            "Epoch 117/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3818\n",
            "Epoch 118/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3728\n",
            "Epoch 119/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3742\n",
            "Epoch 120/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3761\n",
            "Epoch 121/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3740\n",
            "Epoch 122/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3791\n",
            "Epoch 123/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3779\n",
            "Epoch 124/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3781\n",
            "Epoch 125/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3785\n",
            "Epoch 126/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3819\n",
            "Epoch 127/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3809\n",
            "Epoch 128/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3814\n",
            "Epoch 129/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3809\n",
            "Epoch 130/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3828\n",
            "Epoch 131/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3838\n",
            "Epoch 132/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3783\n",
            "Epoch 133/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3826\n",
            "Epoch 134/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3884\n",
            "Epoch 135/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3852\n",
            "Epoch 136/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3855\n",
            "Epoch 137/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3841\n",
            "Epoch 138/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3848\n",
            "Epoch 139/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3892\n",
            "Epoch 140/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3864\n",
            "Epoch 141/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3857\n",
            "Epoch 142/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3850\n",
            "Epoch 143/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3855\n",
            "Epoch 144/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3869\n",
            "Epoch 145/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3895\n",
            "Epoch 146/150\n",
            "172/172 [==============================] - 14s 76ms/step - loss: 0.3898\n",
            "Epoch 147/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3908\n",
            "Epoch 148/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3938\n",
            "Epoch 149/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3941\n",
            "Epoch 150/150\n",
            "172/172 [==============================] - 14s 77ms/step - loss: 0.3923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa-ymW2FrncM"
      },
      "source": [
        "**Loading the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2vQUXjqrpX6"
      },
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW9t_s1Yrra6"
      },
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\r\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUlLjMEurtE_"
      },
      "source": [
        "# checkpoint_num = 10\r\n",
        "# model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\r\n",
        "# model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL4nlKZurust"
      },
      "source": [
        "**Generating Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEL3aVRxrvmm"
      },
      "source": [
        "def generate_text(model, start_string):\r\n",
        "  # Evaluation step (generating text using the learned model)\r\n",
        "\r\n",
        "  # Number of characters to generate\r\n",
        "  num_generate = 800\r\n",
        "\r\n",
        "  # Converting our start string to numbers (vectorizing)\r\n",
        "  input_eval = [char2idx[s] for s in start_string]\r\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\r\n",
        "\r\n",
        "  # Empty string to store our results\r\n",
        "  text_generated = []\r\n",
        "\r\n",
        "  # Low temperatures results in more predictable text.\r\n",
        "  # Higher temperatures results in more surprising text.\r\n",
        "  # Experiment to find the best setting.\r\n",
        "  temperature = 1.0\r\n",
        "\r\n",
        "  # Here batch size == 1\r\n",
        "  model.reset_states()\r\n",
        "  for i in range(num_generate):\r\n",
        "      predictions = model(input_eval)\r\n",
        "      \r\n",
        "      # remove the batch dimension\r\n",
        "      predictions = tf.squeeze(predictions, 0)\r\n",
        "\r\n",
        "      # using a categorical distribution to predict the character returned by the model\r\n",
        "      predictions = predictions / temperature\r\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\r\n",
        "\r\n",
        "      # We pass the predicted character as the next input to the model\r\n",
        "      # along with the previous hidden state\r\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\r\n",
        "\r\n",
        "      text_generated.append(idx2char[predicted_id])\r\n",
        "\r\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W0o2tE4rz-C",
        "outputId": "d187dfca-0d23-4ae9-e8f3-8824c5813e2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "inp = input(\"Type a starting string: \")\r\n",
        "print(generate_text(model, inp))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type a starting string: Frank\n",
            "Franks that fell, have not\n",
            "My guiled all for willing.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "I will be slanderets to my demand.\n",
            "\n",
            "ROMEO:\n",
            "Would all the ruin of doods!\n",
            "What, do I thank thee, Is thy news from her, hence can help from you to Angelo.\n",
            "Now when the sea was a man-child\n",
            "make you, good night; and therefore part the Lady Valeria.\n",
            "But now I see; or if you be much resolved\n",
            "You may be heard that but man die the one\n",
            "Where I was nothing.\n",
            "\n",
            "DION:\n",
            "If they can curse them all my heart to see this captives;\n",
            "The dukedom yet have bleeding thee,\n",
            "For but of my report.\n",
            "\n",
            "COMINIUS:\n",
            "Who shall hath prevent them, and they shall straight\n",
            "five wondering: or, if lions drop is lessor themselves? what, what?--\n",
            "With all god prick mon's charge?\n",
            "Telling this man that great dewren here do fight.\n",
            "\n",
            "MERCUTIO:\n",
            "Nay, and for that sin\n",
            "Out three h\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}